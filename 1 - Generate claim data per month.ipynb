{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from itertools import islice\n",
    "from collections import namedtuple\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "from pyspark.sql.types import BooleanType, DateType, FloatType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import udf, struct, date_add, expr, col, posexplode, months_between, trunc, datediff\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get monthly SAS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "datafiles = [f for f in listdir(\"../MonthlyData\") if isfile(join(\"../MonthlyData\", f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_start_date = udf(lambda svc_dt: date.fromisoformat(svc_dt[0:4] + \"-\" + svc_dt[4:6] + \"-\" + svc_dt[6:8]), DateType())\n",
    "\n",
    "round_int = udf(lambda x: round(try_float(x)), IntegerType())\n",
    "\n",
    "create_dose = udf(lambda quantity, daysDispence, strength, mmeFactor: (try_float(quantity)/(round(try_float(daysDispence)) + 1)) * try_float(strength) * try_float(mmeFactor), FloatType()) \n",
    "\n",
    "def try_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    except TypeError:\n",
    "        return 0\n",
    "    \n",
    "def is_valid(daysSupplied):\n",
    "    if not daysSupplied:\n",
    "        return False\n",
    "    if(daysSupplied == ''): # day supplyed empty\n",
    "        return False\n",
    "    if(round(try_float(daysSupplied)) == 0): # day supplyed == 0\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split_claim_by_effective_month(claim):\n",
    "    months = [dt.strftime(\"%Y%m\") for dt in rrule(MONTHLY, dtstart=claim.start_date, until=claim.end_date)]\n",
    "    \n",
    "    for m in months:\n",
    "        yield Claim(m, claim.start_date, claim.end_date, claim.claim_id, claim.patient_id, claim.provider_id, claim.product_id, claim.quantity, claim.mme_factor, claim.strength, claim.daysDispence, claim.dose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into monthly claim files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sc.setCheckpointDir(dirName=\"../MonthlySpark/checkpoint\")\n",
    "#totalClaims = None\n",
    "dataFilesToRun = datafiles[-1:]\n",
    "for dataFile in dataFilesToRun:\n",
    "    print(\"Processing file: \" + dataFile)\n",
    "    data = spark.read.load(\"../MonthlyData/\" + dataFile, format=\"csv\", sep=\"\\t\", inferSchema=\"false\", header=\"true\")\n",
    "    filter_data_func = udf(is_valid, BooleanType())\n",
    "\n",
    "    data = data.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "    data = data.filter(filter_data_func(data.DAYS_SUPPLY_CNT)).checkpoint().persist(StorageLevel.DISK_ONLY)\\\n",
    "            .withColumn('DAYS_SUPPLY_CNT_int', round_int(data.DAYS_SUPPLY_CNT))\\\n",
    "            .withColumn(\"START_DATE\", create_start_date(data.SVC_DT)).checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "    \n",
    "    #data.count()\n",
    "    data = data.withColumn('END_DATE', expr('date_add(START_DATE, DAYS_SUPPLY_CNT_int)'))\\\n",
    "        .withColumn('DOSE', ((col('DSPNSD_QTY')/(col('DAYS_SUPPLY_CNT_int')+1.0)) * col('STRENGTH') * col('MME_Conversion_Factor')))\\\n",
    "        .drop('SVC_DT', 'DSPNSD_QTY', 'DAYS_SUPPLY_CNT', 'MME_Conversion_Factor', 'STRENGTH', 'DAYS_SUPPLY_CNT_int')\\\n",
    "        .checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "        \n",
    "    #data.count()\n",
    "    data = data.withColumn(\"monthsDiff\", months_between(trunc(\"END_DATE\",'month'), trunc(\"START_DATE\", 'month')))\\\n",
    "        .withColumn(\"repeat\", expr(\"split(repeat(',', monthsDiff), ',')\"))\\\n",
    "        .checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "        \n",
    "    #data.count()\n",
    "    data = data.select(\"*\", posexplode(\"repeat\").alias(\"Effective_Month\", \"val\"))\\\n",
    "        .withColumn(\"Effective_Month\", expr(\"trunc(add_months(START_DATE, Effective_Month), 'month')\"))\\\n",
    "        .drop('monthsDiff','repeat','val')\\\n",
    "        .checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "    #data.count()\n",
    "    \n",
    "    if not totalClaims:\n",
    "        totalClaims = data\n",
    "    else:\n",
    "        totalClaims = totalClaims.union(data)\n",
    "    \n",
    "    totalClaims = totalClaims.checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "    currentMonth = dataFile.split('_')[1].split('.')[0]\n",
    "    currentMonth = date.fromisoformat(currentMonth[0:4] + \"-\" + currentMonth[4:6] + \"-01\")\n",
    "    fileName = \"../MonthlySpark/\" + currentMonth.strftime('%Y%m') + \".parquet\"\n",
    "    print(\"Writing file: \" + fileName + \" for month: \" + currentMonth.strftime('%Y%m'))\n",
    "    claimsToWrite = totalClaims.where(expr('datediff(Effective_Month,cast(\\\"'+currentMonth.strftime('%Y-%m-%d')+'\\\" as date)) == 0'))\n",
    "    claimsToWrite.checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "        \n",
    "    claimsToWrite.write.parquet(fileName)\n",
    "    totalClaims = totalClaims.where(expr('datediff(Effective_Month,cast(\\\"'+currentMonth.strftime('%Y-%m-%d')+'\\\" as date)) <> 0'))\n",
    "    claimsToWrite.unpersist()\n",
    "    del claimsToWrite\n",
    "    data.unpersist()\n",
    "    del data\n",
    "    \n",
    "remainingMonths = sorted(totalClaims.select(totalClaims.Effective_Month).distinct().collect())\n",
    "     \n",
    "for currentMonth in remainingMonths:\n",
    "    currentMonthStr = currentMonth.Effective_Month.strftime('%Y%m')\n",
    "    currentMonthDayStr = currentMonth.Effective_Month.strftime('%Y-%m-%d')\n",
    "    print(\"Processing month: \" + currentMonthStr)\n",
    "    fileName = \"../MonthlySpark/\" + currentMonthStr + \".parquet\"\n",
    "    claimsToWrite = totalClaims.where(expr('datediff(Effective_Month,cast(\\\"'+currentMonthDayStr+'\\\" as date)) == 0'))\n",
    "    claimsToWrite.checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "    \n",
    "    print(\"Writing file: \" + fileName + \" for month: \" + currentMonthStr)\n",
    "    claimsToWrite.write.parquet(fileName)\n",
    "    totalClaims = totalClaims.where(expr('datediff(Effective_Month,cast(\\\"'+currentMonthDayStr+'\\\" as date)) <> 0')).checkpoint().persist(StorageLevel.DISK_ONLY)\n",
    "    claimsToWrite.unpersist()\n",
    "    del claimsToWrite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
